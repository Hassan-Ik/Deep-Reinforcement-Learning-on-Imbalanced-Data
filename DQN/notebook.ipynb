{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from collections import deque\n",
    "import random\n",
    "from qnetwork import DQNetwork\n",
    "from dataset import Cifar10ImageDataset\n",
    "from agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, gamma: float, epsilon: float, learning_rate: float):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration-exploitation trade-off\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "        \n",
    "        # Main Q-network\n",
    "        self.model = self.build_network()\n",
    "\n",
    "        # Target Q-network\n",
    "        self.target_model = self.build_network()\n",
    "        \n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def build_network(self):\n",
    "        # Input layer of the network\n",
    "        input_layer = tf.keras.layers.Input(shape = self.state_size)\n",
    "\n",
    "        # Convolution Layers\n",
    "        conv2 = tf.keras.layers.Conv2D(32, 5, strides=2, activation=tf.nn.relu)(input_layer)\n",
    "        conv2 = tf.keras.layers.Conv2D(32, 5, strides=2, activation=tf.nn.relu)(conv2)\n",
    "        flatten = tf.keras.layers.Flatten()(conv2)\n",
    "        outputs = tf.keras.layers.Dense(self.action_size, activation=\"softmax\")(flatten)\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
    "        \n",
    "        # Compiling Deep Q Network Model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='categorical_crossentropy')\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, network, dataset, state_size, action_size, memory, epsilon):\n",
    "        self.network = network\n",
    "        self.dataset = dataset\n",
    "        self.memory = memory\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1024)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, terminal):\n",
    "        self.memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        q_values = self.network.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def get_reward_and_terminal(self, label, action):\n",
    "        terminal = 0\n",
    "        if action == label:\n",
    "            reward = self.dataset.reward_set[label]\n",
    "        else:\n",
    "            reward = - self.dataset.reward_set[label]\n",
    "            # End of an episode if the agent misjudgement about a minority class\n",
    "            if label in self.dataset.minority_classes:\n",
    "                terminal = 1\n",
    "        return reward, terminal\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, terminal in minibatch:\n",
    "            target = self.network.model.predict(state)\n",
    "            if terminal:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.network.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.network.gamma * np.amax(t)\n",
    "\n",
    "            print(\"Target is \", target)\n",
    "            self.network.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        self.update_epsilon()\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        self.network.model.save_weights(save_path)\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            int_val = np.random.randint(0, self.dataset.length_of_dataset)\n",
    "            state_batch = self.dataset.dataset.take(int_val)\n",
    "            for state, label in list(state_batch):\n",
    "                action = self.act(state)\n",
    "                next_state = self.dataset.dataset.take(np.random.randint(0, self.dataset.length_of_dataset))\n",
    "                next_state = tuple(next_state)[0][0]\n",
    "                \n",
    "                print(f\"Action is {action} and label is {label}\")\n",
    "                reward, terminal = self.get_reward_and_terminal(label, action)\n",
    "                self.remember(state, action, reward, next_state, terminal)\n",
    "                state = next_state\n",
    "\n",
    "                if terminal == 1:\n",
    "                    self.network.update_target_network()\n",
    "                    print(\"Episode: {}, Reward: {}\".format(episode, reward))\n",
    "                    break\n",
    "\n",
    "                if len(self.memory) > self.dataset.batch_size:\n",
    "                    self.replay(self.dataset.batch_size)\n",
    "        # for episode in range(num_episodes):\n",
    "        #     int_val = np.random.randint(0, len(self.dataset.training_data_batches))\n",
    "        #     state_batch = self.dataset.training_data_batches.take(int_val)\n",
    "        #     for states, labels in tuple(state_batch):\n",
    "        #         for index, val in enumerate(labels):\n",
    "        #             state = states[index]\n",
    "        #             action = self.act(state.numpy())\n",
    "        #             next_state = states[np.random.randint(0, len(states))]\n",
    "                    \n",
    "        #             print(f\"Action is {action} and label is {labels[index]}\")\n",
    "        #             reward, terminal = self.get_reward_and_terminal(labels[index], action)\n",
    "        #             self.remember(state, action, reward, next_state, terminal)\n",
    "        #             state = next_state\n",
    "\n",
    "        #             if terminal == 1:\n",
    "        #                 self.network.update_target_network()\n",
    "        #                 print(\"Episode: {}, Reward: {}\".format(episode, reward))\n",
    "        #                 break\n",
    "\n",
    "        #             if len(self.memory) > self.dataset.batch_size:\n",
    "        #                 self.replay(self.dataset.batch_size)\n",
    "                \n",
    "        # Testing the model\n",
    "        # total_reward = 0\n",
    "        # for test_state in X_test:\n",
    "        #     test_state = np.reshape(test_state, [1, state_size])\n",
    "        #     action = self.act(test_state)\n",
    "        #     reward = 1 if y_test[np.argmax(test_state)] == 1 else -1\n",
    "        #     total_reward += reward\n",
    "\n",
    "        # print(\"Test Accuracy: {:.2%}\".format(total_reward / len(X_test)))\n",
    "        \n",
    "    def evaluate(self, train_label, train_prediction, val_label, val_prediction, step, show_phase=\"Both\"):\n",
    "        # Calculate f1 score of each class and weighted macro average\n",
    "        print(\"train_step : {}, epsilon : {:.3f}\".format(step, self.epsilon))\n",
    "        if show_phase == \"Both\":\n",
    "            phase = [\"Train Data.\", \"Validation Data.\"]\n",
    "            labels = [train_label, val_label]\n",
    "            predictions = [train_prediction, val_prediction]\n",
    "        elif show_phase == \"Train\":\n",
    "            phase = [\"Train Data.\"]\n",
    "            labels = [train_label]\n",
    "            predictions = [train_prediction]\n",
    "        elif show_phase == \"Validation\":\n",
    "            phase = [\"Validation Data.\"]\n",
    "            labels = [val_label]\n",
    "            predictions = [val_prediction]\n",
    "\n",
    "        for idx, (label, prediction) in enumerate(zip(labels, predictions)):\n",
    "            f1_all_cls = metrics.f1_score(label, prediction, average=None)\n",
    "            f1_macro_avg = metrics.f1_score(label, prediction, average='weighted')\n",
    "            print(\"\\t\\t {:<20} f1-score of \".format(phase[idx]), end=\"\")\n",
    "            for i, f1 in enumerate(f1_all_cls):\n",
    "                print(\"class {} : {:.3f}\".format(i, f1), end=\", \")\n",
    "            print(\"weighted macro avg : {:.3f}\".format(f1_macro_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Cifar10ImageDataset:\n",
    "    def __init__(self, batch_size=32):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.create_dataset()\n",
    "        self.get_labels_counts()\n",
    "        self.get_rho()\n",
    "        self.get_minority_classes()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        \n",
    "        X = np.concatenate([X_train, X_test])\n",
    "        y = np.concatenate([y_train, y_test])\n",
    "        \n",
    "        # Convert labels to integers\n",
    "        y = y.flatten()\n",
    "\n",
    "        # Create a TensorFlow Dataset from the CIFAR-10 data\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        \n",
    "        # We are going to get 25% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        self.no_of_minority_classes_to_get = int(np.round(len(np.unique(y)) * 0.25))\n",
    "        \n",
    "        \n",
    "        # Specify the percentage of label 2 data to remove\n",
    "        percentage_to_remove = 0.9\n",
    "        for min_ in range(self.no_of_minority_classes_to_get):\n",
    "            # Use the filter_data function to create a new dataset with filtered data\n",
    "            dataset = dataset.filter(lambda x, y: tf.py_function(filter_data, inp=[x, y, min_, percentage_to_remove], Tout=tf.bool))\n",
    "            percentage_to_remove -= 0.1\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.length_of_dataset = len(list(self.dataset.as_numpy_iterator()))\n",
    "    \n",
    "    # Define a function to filter out data with label 2 based on a percentage\n",
    "    def filter_data(self, image, label, label_to_drop, percentage_to_remove):\n",
    "        # Assuming label 2 corresponds to the class you want to remove\n",
    "        if label == label_to_drop.numpy() and tf.random.uniform(()) < percentage_to_remove:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_labels_counts(self):\n",
    "        labels_dataset = self.dataset.map(lambda x, y: y)\n",
    "        \n",
    "        # Convert the labels dataset to a NumPy array\n",
    "        labels_array = list(labels_dataset.as_numpy_iterator())\n",
    "\n",
    "        # Get unique labels and their counts\n",
    "        self.unique_labels, self.label_counts = np.unique(labels_array, return_counts=True)\n",
    "        \n",
    "        return label_counts\n",
    "        \n",
    "    def get_minority_classes(self):\n",
    "        unique_labels_counts_dict = dict(zip(self.unique_labels, self.label_counts))\n",
    "        unique_labels_counts_dict = sorted(unique_labels_counts_dict.items())\n",
    "        \n",
    "        self.minority_classes = []\n",
    "        for i in range(self.no_of_minority_classes_to_get):\n",
    "            self.minority_classes.append(unique_labels_counts_dict[i][0])\n",
    "\n",
    "    def get_rho(self):\n",
    "        \"\"\"\n",
    "        In the two-class dataset problem, this paper has proven that the best performance is achieved when the reciprocal of the ratio of the number of data is used as the reward function.\n",
    "        In this code, the result of this paper is extended to multi-class by creating a reward function with the reciprocal of the number of data for each class.\n",
    "        \"\"\"\n",
    "        labels_counts = self.get_labels_counts()\n",
    "        raw_reward_set = 1 / labels_counts\n",
    "        self.reward_set = np.round(raw_reward_set / np.linalg.norm(raw_reward_set), 6)\n",
    "        print(\"\\nReward for each class.\")\n",
    "        for cl_idx, cl_reward in enumerate(self.reward_set):\n",
    "            print(\"\\t- Class {} : {:.6f}\".format(cl_idx, cl_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_csv(folder_path: str, file_path: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = shuffle(df, random_state=42)\n",
    "    df[\"filepath\"] = folder_path + df[\"image_id\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../digit-recognizer/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_images = np.concatenate([X_train, X_test])\n",
    "train_labels = np.concatenate([y_train, y_test])\n",
    "# Convert labels to integers\n",
    "train_labels = train_labels.flatten()\n",
    "\n",
    "# Create a TensorFlow Dataset from the CIFAR-10 data\n",
    "cifar_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Define a function to filter out data with label 2 based on a percentage\n",
    "def filter_data(image, label, label_to_drop, percentage_to_remove):\n",
    "    # Assuming label 2 corresponds to the class you want to remove\n",
    "    if label == label_to_drop.numpy() and tf.random.uniform(()) < percentage_to_remove:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Specify the percentage of label 2 data to remove\n",
    "percentage_to_remove = 0.9 # Example: Remove 30% of label 2 data\n",
    "\n",
    "# Use the filter_data function to create a new dataset with filtered data\n",
    "filtered_dataset = cifar_dataset.filter(lambda x, y: tf.py_function(filter_data, inp=[x, y, 2, percentage_to_remove], Tout=tf.bool))\n",
    "# Use the filter_data function to create a new dataset with filtered data\n",
    "filtered_dataset = filtered_dataset.filter(lambda x, y: tf.py_function(filter_data, inp=[x, y, 3, percentage_to_remove], Tout=tf.bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FilterDataset element_spec=(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49255"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filtered_dataset.as_numpy_iterator()))\n",
    "filtered_dataset.reduce(0, lambda x, _: x + 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Count 5000\n",
      "Label 1: Count 5000\n",
      "Label 2: Count 473\n",
      "Label 3: Count 487\n",
      "Label 4: Count 5000\n",
      "Label 5: Count 5000\n",
      "Label 6: Count 5000\n",
      "Label 7: Count 5000\n",
      "Label 8: Count 5000\n",
      "Label 9: Count 5000\n"
     ]
    }
   ],
   "source": [
    "labels_dataset = filtered_dataset.map(lambda x, y: y)\n",
    "\n",
    "# Convert the labels dataset to a NumPy array\n",
    "labels_array = list(labels_dataset.as_numpy_iterator())\n",
    "\n",
    "# Get unique labels and their counts\n",
    "unique_labels, label_counts = np.unique(labels_array, return_counts=True)\n",
    "\n",
    "# Create a dictionary mapping unique labels to their counts\n",
    "label_count_dict = dict(zip(unique_labels, label_counts))\n",
    "\n",
    "# Print the unique labels and their counts\n",
    "for label, count in label_count_dict.items():\n",
    "    print(f\"Label {label}: Count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       " \n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       " \n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       " \n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       " \n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]], dtype=uint8),\n",
       " 6)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filtered_dataset.as_numpy_iterator())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the filtered dataset: 45967\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in the filtered dataset:\", len(list(filtered_dataset.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the percentage of label 2 data to remove\n",
    "percentage_to_remove = 0.8\n",
    "\n",
    "# Use the filter_data function to create a new dataset with filtered data\n",
    "filtered_dataset = cifar_dataset.filter(lambda x, y: tf.py_function(filter_data, inp=[x, y, 2, percentage_to_remove], Tout=tf.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "tf.Tensor(6, shape=(), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "for img, label in filtered_dataset.take(1):\n",
    "    print(img.shape)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Convert labels to integers\n",
    "train_labels = train_labels.flatten()\n",
    "\n",
    "# Create a TensorFlow Dataset from the CIFAR-10 data\n",
    "cifar_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Define a function to filter out data with label 2 based on a percentage\n",
    "def filter_data(image, label, percentage_to_remove):\n",
    "    # Assuming label 2 corresponds to the class you want to remove\n",
    "    if label == 2 and tf.random.uniform(()) < percentage_to_remove:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Specify the percentage of label 2 data to remove\n",
    "percentage_to_remove = 0.3  # Example: Remove 30% of label 2 data\n",
    "\n",
    "# Use the filter_data function to create a new dataset with filtered data\n",
    "filtered_dataset = cifar_dataset.filter(lambda x, y: tf.py_function(filter_data, inp=[x, y, percentage_to_remove], Tout=tf.bool))\n",
    "\n",
    "# Print the number of samples in the filtered dataset\n",
    "print(\"Number of samples in the filtered dataset:\", len(list(filtered_dataset.as_numpy_iterator())))\n",
    "\n",
    "# Optionally, iterate over the filtered dataset to inspect the data\n",
    "for image, label in filtered_dataset.take(5):\n",
    "    print(\"Label:\", label.numpy())\n",
    "    # Additional processing or visualization code can be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_some_labels(X_train, y_train):\n",
    "    for train_data in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reading_csv(\"../cassava-leaf-disease-classification/train_images/\", \"../cassava-leaf-disease-classification/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_some_labels(df, label):\n",
    "    label_ = df[df['label'] == label]\n",
    "    n_to_drop = label_.shape[0]//3\n",
    "    index_to_drop = label_.sample(n_to_drop).index\n",
    "    df = df.drop(index_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_some_labels(df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reward for each class.\n",
      "\t- Class 0 : 0.066644\n",
      "\t- Class 1 : 0.066644\n",
      "\t- Class 2 : 0.704483\n",
      "\t- Class 3 : 0.684231\n",
      "\t- Class 4 : 0.066644\n",
      "\t- Class 5 : 0.066644\n",
      "\t- Class 6 : 0.066644\n",
      "\t- Class 7 : 0.066644\n",
      "\t- Class 8 : 0.066644\n",
      "\t- Class 9 : 0.066644\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar10ImageDataset(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "network = DQNetwork(state_size=(32, 32, 3), action_size=10, gamma = 0.95, epsilon = 1.0, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(network, dataset, state_size=(32, 32, 3), action_size = 10, memory=deque(maxlen=2000), epsilon=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action is 7 and label is 6\n",
      "Action is 6 and label is 9\n",
      "Action is 6 and label is 9\n",
      "Action is 5 and label is 4\n",
      "Action is 2 and label is 1\n",
      "Episode: 0, Reward: -0.066644\n",
      "Action is 1 and label is 6\n",
      "Action is 9 and label is 9\n",
      "Action is 0 and label is 9\n",
      "Action is 9 and label is 4\n",
      "Action is 0 and label is 2\n",
      "Action is 2 and label is 7\n",
      "Action is 9 and label is 8\n",
      "Action is 9 and label is 3\n",
      "Action is 8 and label is 4\n",
      "Action is 7 and label is 7\n",
      "Action is 3 and label is 7\n",
      "Action is 5 and label is 2\n",
      "Action is 8 and label is 9\n",
      "Action is 0 and label is 9\n",
      "Action is 2 and label is 9\n",
      "Action is 2 and label is 3\n",
      "Action is 7 and label is 2\n",
      "Action is 6 and label is 6\n",
      "Action is 9 and label is 4\n",
      "Action is 4 and label is 3\n"
     ]
    }
   ],
   "source": [
    "agent.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
